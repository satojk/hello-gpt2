import torch

from transformers import GPT2Tokenizer, GPT2LMHeadModel, GPT2Model

GPT2_MAX_SEQ_LEN = 1024

class GPT2Engine(object):

    def __init__(self) -> None:
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        self.tokenizer.pad_token = self.tokenizer.eos_token


    def tokenize(self, text: str) -> torch.Tensor:
        return self.tokenizer.encode(text, return_tensors='pt').to(self.device)


    def detokenize(self, tokens: torch.Tensor) -> str:
        return self.tokenizer.decode(tokens, clean_up_tokenization_spaces=True)


class GPT2LMEngine(GPT2Engine):

    def __init__(self) -> None:
        super().__init__()
        self.model = GPT2LMHeadModel.from_pretrained(
                'gpt2',
                pad_token_id=self.tokenizer.eos_token_id,
                temperature=1)
        if self.device == 'cuda':
            self.model.eval().cuda()
        else:
            self.model.eval()


    def generate_text(self, prompt: str, num_tokens: int=5, num_candidates: int=1) -> list:
        input_token_ids = self.tokenize(prompt)
        if input_token_ids.shape[1] > GPT2_MAX_SEQ_LEN:
            raise Exception('Prompt too long.')

        output_token_ids = self.model.generate(input_ids=input_token_ids,
                                               max_length=len(input_token_ids[0]) + num_tokens,
                                               do_sample=False)
        return self.detokenize(output_token_ids[0])


    def predict_document(self, document: str, window_size: int=25, batch_size: int=100, ignore_set: iter=[]) -> tuple:
        '''
        Slide a window of window_size tokens across an entire document. At each
        step, use the window contents as a prompt, and generate a prediction
        for the next token (except when the token to be predicted is in
        ignore_set)

        Return a pair of (predictions, true_tokens), where predictions is a
        large 2D tensor where each row is a prediction (window_size tokens from
        the document, followed by a final token generated by GPT2), and
        true_tokens is the transpose of the original document tokenized, minus
        the first window_size tokens (that is, each row consists of a single
        token, namely the true token that GPT2 should have predicted at each
        step).
        '''
        input_token_ids = self.tokenize(document)
        if input_token_ids.shape[1] < window_size - 1:
            raise ValueError('Window contains entire document.')
        prompts = input_token_ids.squeeze(0).unfold(0, window_size, 1)[:-1]
        true_tokens = input_token_ids[:, window_size:].view(-1, 1)
        for token_id in ignore_set:
            should_keep_prompt = true_tokens != token_id
            prompts = prompts[should_keep_prompt[:, 0]]
            true_tokens = true_tokens[should_keep_prompt[:, 0]]
        batch_beg = 0
        batch_end = batch_size
        predictions = []
        while batch_beg < prompts.shape[0]:
            batch = prompts[batch_beg:batch_end]
            predictions.append(self.model.generate(input_ids=batch,
                                                   max_length=window_size+1,
                                                   do_sample=False))
            batch_beg += batch_size
            batch_end += batch_size
        return (torch.cat(predictions, dim=0),
                true_tokens)


class GPT2ModelEngine(GPT2Engine):

    def __init__(self) -> None:
        super().__init__()
        self.model = GPT2Model.from_pretrained(
                'gpt2',
                pad_token_id=self.tokenizer.eos_token_id,
                temperature=1)
        if self.device == 'cuda':
            self.model.eval().cuda()
        else:
            self.model.eval()


    def get_token_embeddings(self, input_token_ids: torch.Tensor) -> torch.Tensor:
        with torch.no_grad():
            return self.model(input_token_ids).last_hidden_state


    def get_text_embeddings(self, prompt: str) -> torch.Tensor:
        return self.get_token_embeddings(self.tokenize(prompt))
